{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14631448,"sourceType":"datasetVersion","datasetId":9346393}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# COMPLETE EMOTION RECOGNITION TRAINING\n# Using Your emotions-dataset\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nimport os\nimport torch\nimport numpy as np\nimport librosa\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    Wav2Vec2ForSequenceClassification,\n    Wav2Vec2FeatureExtractor,\n    TrainingArguments,\n    Trainer\n)\nfrom sklearn.metrics import accuracy_score, f1_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# CONFIGURATION\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nMODEL_NAME = \"facebook/wav2vec2-xls-r-300m\"\nOUTPUT_DIR = \"./emotion-recognition-model\"\nNUM_EPOCHS = 15\nBATCH_SIZE = 4\nLEARNING_RATE = 2e-5\nMAX_LENGTH = 5 * 16000  # 5 seconds at 16kHz\n\n# Emotion labels based on your dataset structure\nEMOTION_LABELS = {\n    \"angry\": 0,\n    \"happy\": 1,\n    \"hesitant\": 2,\n    \"interested\": 3,\n    \"neutral\": 4\n}\n\nLABEL_TO_ID = EMOTION_LABELS\nID_TO_LABEL = {v: k for k, v in EMOTION_LABELS.items()}\n\nprint(f\"ğŸ¯ Emotion Labels: {EMOTION_LABELS}\")\nprint(f\"ğŸ“Š Number of classes: {len(EMOTION_LABELS)}\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# DATA LOADING\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef load_audio_file(file_path, target_sr=16000):\n    \"\"\"Load and preprocess audio file\"\"\"\n    try:\n        audio, sr = librosa.load(file_path, sr=target_sr)\n        return audio\n    except Exception as e:\n        print(f\"Error loading {file_path}: {e}\")\n        return None\n\ndef load_dataset_from_folders(base_path):\n    \"\"\"\n    Load dataset from your Kaggle folder structure:\n    /kaggle/input/emotions-dataset/Emotions/Angry/*.wav\n    /kaggle/input/emotions-dataset/Emotions/Happy/*.wav\n    etc.\n    \"\"\"\n    data = []\n    emotions_path = base_path / \"Emotions\"\n    \n    print(f\"\\nğŸ“‚ Loading data from: {emotions_path}\")\n    \n    # Map folder names to standardized emotion labels\n    folder_to_emotion = {\n        \"Angry\": \"angry\",\n        \"Happy\": \"happy\",\n        \"Hesitant\": \"hesitant\",\n        \"Interested\": \"interested\",\n        \"Neutral\": \"neutral\"\n    }\n    \n    for folder_name, emotion in folder_to_emotion.items():\n        emotion_folder = emotions_path / folder_name\n        \n        if not emotion_folder.exists():\n            print(f\"âš ï¸  Warning: {emotion_folder} not found!\")\n            continue\n        \n        # Find all audio files (wav, mp3, etc.)\n        audio_files = list(emotion_folder.glob(\"*.wav\")) + \\\n                     list(emotion_folder.glob(\"*.mp3\")) + \\\n                     list(emotion_folder.glob(\"*.m4a\"))\n        \n        print(f\"  {emotion}: {len(audio_files)} files\")\n        \n        for audio_file in audio_files:\n            data.append({\n                \"path\": str(audio_file),\n                \"emotion\": emotion,\n                \"label\": EMOTION_LABELS[emotion]\n            })\n    \n    print(f\"\\nâœ… Total samples loaded: {len(data)}\")\n    return data\n\n# Load your dataset\nDATASET_PATH = Path(\"/kaggle/input/emotions-dataset\")\nall_data = load_dataset_from_folders(DATASET_PATH)\n\n# Split into train/val/test\ntrain_data, temp_data = train_test_split(all_data, test_size=0.3, random_state=42, stratify=[d[\"label\"] for d in all_data])\nval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=[d[\"label\"] for d in temp_data])\n\nprint(f\"\\nğŸ“Š Dataset splits:\")\nprint(f\"  Train: {len(train_data)} samples\")\nprint(f\"  Validation: {len(val_data)} samples\")\nprint(f\"  Test: {len(test_data)} samples\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# FEATURE EXTRACTION\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME)\n\ndef preprocess_function(examples):\n    \"\"\"Process audio files for the model\"\"\"\n    audio_arrays = []\n    \n    for path in examples[\"path\"]:\n        audio = load_audio_file(path)\n        if audio is not None:\n            # Pad or truncate to MAX_LENGTH\n            if len(audio) > MAX_LENGTH:\n                audio = audio[:MAX_LENGTH]\n            else:\n                audio = np.pad(audio, (0, MAX_LENGTH - len(audio)))\n            audio_arrays.append(audio)\n        else:\n            # If file fails to load, use silence\n            audio_arrays.append(np.zeros(MAX_LENGTH))\n    \n    # Extract features\n    inputs = feature_extractor(\n        audio_arrays,\n        sampling_rate=16000,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n    \n    return {\n        \"input_values\": inputs.input_values,\n        \"labels\": examples[\"label\"]\n    }\n\n# Create HuggingFace datasets\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\ntest_dataset = Dataset.from_list(test_data)\n\ndataset_dict = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": val_dataset,\n    \"test\": test_dataset\n})\n\n# Apply preprocessing\nprint(\"\\nğŸ”„ Preprocessing datasets...\")\ndataset_dict = dataset_dict.map(\n    preprocess_function,\n    batched=True,\n    batch_size=8,\n    remove_columns=[\"path\", \"emotion\"]\n)\n\nprint(\"âœ… Preprocessing complete!\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# MODEL SETUP\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=len(EMOTION_LABELS),\n    label2id=LABEL_TO_ID,\n    id2label=ID_TO_LABEL,\n    ignore_mismatched_sizes=True\n)\n\n# Freeze feature extractor\nmodel.freeze_feature_encoder()\n\nprint(f\"\\nğŸ¤– Model loaded: {MODEL_NAME}\")\nprint(f\"ğŸ“Š Number of parameters: {model.num_parameters():,}\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# TRAINING CONFIGURATION\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef compute_metrics(eval_pred):\n    \"\"\"Compute accuracy and F1 score\"\"\"\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average='weighted')\n    \n    return {\n        \"accuracy\": acc,\n        \"f1\": f1\n    }\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=LEARNING_RATE,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    num_train_epochs=NUM_EPOCHS,\n    warmup_ratio=0.1,\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    push_to_hub=False,\n    fp16=True,  # Use mixed precision for faster training\n    save_total_limit=2,\n    report_to=[\"tensorboard\"]\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset_dict[\"train\"],\n    eval_dataset=dataset_dict[\"validation\"],\n    compute_metrics=compute_metrics,\n)\n\nprint(\"\\nğŸš€ Starting training...\")\nprint(f\"â±ï¸  Epochs: {NUM_EPOCHS}\")\nprint(f\"ğŸ“¦ Batch size: {BATCH_SIZE}\")\nprint(f\"ğŸ“ˆ Learning rate: {LEARNING_RATE}\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# TRAIN THE MODEL\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ntrainer.train()\n\nprint(\"\\nâœ… Training complete!\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# EVALUATION\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nprint(\"\\nğŸ“Š Evaluating on test set...\")\ntest_results = trainer.evaluate(dataset_dict[\"test\"])\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL TEST RESULTS\")\nprint(\"=\"*60)\nfor key, value in test_results.items():\n    print(f\"{key}: {value:.4f}\")\nprint(\"=\"*60)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# SAVE MODEL\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nfinal_model_path = f\"{OUTPUT_DIR}/final\"\ntrainer.save_model(final_model_path)\nfeature_extractor.save_pretrained(final_model_path)\n\nprint(f\"\\nğŸ’¾ Model saved to: {final_model_path}\")\nprint(\"\\nğŸ‰ All done! Your emotion recognition model is ready!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T17:15:20.070235Z","iopub.execute_input":"2026-01-29T17:15:20.070853Z","iopub.status.idle":"2026-01-29T18:42:08.955303Z","shell.execute_reply.started":"2026-01-29T17:15:20.070824Z","shell.execute_reply":"2026-01-29T18:42:08.954606Z"}},"outputs":[{"name":"stderr","text":"2026-01-29 17:15:36.709056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769706936.903692      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769706936.958986      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769706937.422881      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769706937.422929      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769706937.422932      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769706937.422934      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ¯ Emotion Labels: {'angry': 0, 'happy': 1, 'hesitant': 2, 'interested': 3, 'neutral': 4}\nğŸ“Š Number of classes: 5\n\nğŸ“‚ Loading data from: /kaggle/input/emotions-dataset/Emotions\n  angry: 499 files\n  happy: 500 files\n  hesitant: 502 files\n  interested: 498 files\n  neutral: 500 files\n\nâœ… Total samples loaded: 2499\n\nğŸ“Š Dataset splits:\n  Train: 1749 samples\n  Validation: 375 samples\n  Test: 375 samples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b04a79095124b8693e82d2995237ba0"}},"metadata":{}},{"name":"stdout","text":"\nğŸ”„ Preprocessing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1749 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcf0958a3e53476d874572b7a442c7ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/375 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"181335858cd64fa4b315a44c2d157d1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/375 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92eb2966aaee4ba2b20d67341d682be2"}},"metadata":{}},{"name":"stdout","text":"âœ… Preprocessing complete!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9620aa9987ef4d2fae85261d431d8183"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36ff3f432a0f4f52b1b89f2a1ffa5afa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18f7ef30dec3457fac077352c498bb56"}},"metadata":{}},{"name":"stderr","text":"Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ¤– Model loaded: facebook/wav2vec2-xls-r-300m\nğŸ“Š Number of parameters: 315,702,405\n\nğŸš€ Starting training...\nâ±ï¸  Epochs: 15\nğŸ“¦ Batch size: 4\nğŸ“ˆ Learning rate: 2e-05\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3285' max='3285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3285/3285 1:24:32, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.602900</td>\n      <td>1.599049</td>\n      <td>0.197333</td>\n      <td>0.070883</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.356900</td>\n      <td>1.230649</td>\n      <td>0.597333</td>\n      <td>0.548685</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.908100</td>\n      <td>0.799255</td>\n      <td>0.749333</td>\n      <td>0.710771</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.798300</td>\n      <td>0.646883</td>\n      <td>0.792000</td>\n      <td>0.788147</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.577300</td>\n      <td>0.502910</td>\n      <td>0.810667</td>\n      <td>0.802229</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.533400</td>\n      <td>0.517490</td>\n      <td>0.805333</td>\n      <td>0.804103</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.514800</td>\n      <td>0.561902</td>\n      <td>0.786667</td>\n      <td>0.786075</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.387700</td>\n      <td>0.567250</td>\n      <td>0.762667</td>\n      <td>0.761251</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.215400</td>\n      <td>0.358186</td>\n      <td>0.880000</td>\n      <td>0.880687</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.220000</td>\n      <td>0.391238</td>\n      <td>0.872000</td>\n      <td>0.870476</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.261500</td>\n      <td>0.428890</td>\n      <td>0.856000</td>\n      <td>0.857796</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.209600</td>\n      <td>0.381088</td>\n      <td>0.888000</td>\n      <td>0.889357</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.209500</td>\n      <td>0.394719</td>\n      <td>0.901333</td>\n      <td>0.902359</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.211200</td>\n      <td>0.380860</td>\n      <td>0.898667</td>\n      <td>0.899698</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.113500</td>\n      <td>0.384556</td>\n      <td>0.901333</td>\n      <td>0.902327</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nâœ… Training complete!\n\nğŸ“Š Evaluating on test set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [47/47 00:35]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n============================================================\nFINAL TEST RESULTS\n============================================================\neval_loss: 0.4150\neval_accuracy: 0.8613\neval_f1: 0.8641\neval_runtime: 36.5548\neval_samples_per_second: 10.2590\neval_steps_per_second: 1.2860\nepoch: 15.0000\n============================================================\n\nğŸ’¾ Model saved to: ./emotion-recognition-model/final\n\nğŸ‰ All done! Your emotion recognition model is ready!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nCheck if your model was saved correctly\n\"\"\"\nimport os\nfrom pathlib import Path\n\nprint(\"Checking model directory...\")\nprint(\"=\"*60)\n\n# Check main model directory\nmodel_dir = Path(\"/kaggle/working/emotion-recognition-model\")\n\nif model_dir.exists():\n    print(f\"âœ… Directory exists: {model_dir}\")\n    print(f\"\\nContents of {model_dir}:\")\n    print(\"-\"*60)\n    \n    for item in sorted(model_dir.iterdir()):\n        if item.is_dir():\n            print(f\"  ğŸ“ {item.name}/\")\n            # Check subdirectories\n            for subitem in sorted(item.iterdir())[:5]:  # Show first 5 items\n                print(f\"     - {subitem.name}\")\n            if len(list(item.iterdir())) > 5:\n                print(f\"     ... and {len(list(item.iterdir())) - 5} more files\")\n        else:\n            size = item.stat().st_size / (1024 * 1024)  # Size in MB\n            print(f\"  ğŸ“„ {item.name} ({size:.2f} MB)\")\n    \n    # Check for required files\n    print(\"\\n\" + \"=\"*60)\n    print(\"Checking for required model files:\")\n    print(\"-\"*60)\n    \n    required_files = [\n        \"config.json\",\n        \"preprocessor_config.json\",\n        \"model.safetensors\",  # or pytorch_model.bin\n    ]\n    \n    for file in required_files:\n        file_path = model_dir / file\n        if file_path.exists():\n            print(f\"  âœ… {file}\")\n        else:\n            # Check alternative names\n            if file == \"model.safetensors\":\n                alt_path = model_dir / \"pytorch_model.bin\"\n                if alt_path.exists():\n                    print(f\"  âœ… pytorch_model.bin (alternative)\")\n                else:\n                    print(f\"  âŒ {file} (MISSING!)\")\n            else:\n                print(f\"  âŒ {file} (MISSING!)\")\n    \n    # Check if there's a 'final' subdirectory\n    final_dir = model_dir / \"final\"\n    if final_dir.exists():\n        print(f\"\\nâœ… Found 'final' subdirectory!\")\n        print(f\"\\nContents of {final_dir}:\")\n        print(\"-\"*60)\n        for item in sorted(final_dir.iterdir()):\n            if item.is_file():\n                size = item.stat().st_size / (1024 * 1024)\n                print(f\"  ğŸ“„ {item.name} ({size:.2f} MB)\")\n    else:\n        print(f\"\\nâš ï¸  No 'final' subdirectory found\")\n    \nelse:\n    print(f\"âŒ Directory does NOT exist: {model_dir}\")\n    print(\"\\nâš ï¸  This means the model was NOT saved!\")\n    print(\"\\nPossible reasons:\")\n    print(\"  1. Training did not complete successfully\")\n    print(\"  2. Training is still running\")\n    print(\"  3. Model was saved to a different location\")\n    \n    # Check working directory\n    print(f\"\\nChecking /kaggle/working/ directory:\")\n    print(\"-\"*60)\n    working_dir = Path(\"/kaggle/working\")\n    if working_dir.exists():\n        for item in sorted(working_dir.iterdir()):\n            if item.is_dir():\n                print(f\"  ğŸ“ {item.name}/\")\n            else:\n                size = item.stat().st_size / (1024 * 1024)\n                print(f\"  ğŸ“„ {item.name} ({size:.2f} MB)\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CONCLUSION:\")\nprint(\"=\"*60)\n\nmodel_path = Path(\"/kaggle/working/emotion-recognition-model\")\nfinal_path = model_path / \"final\"\n\nif final_path.exists() and (final_path / \"config.json\").exists():\n    print(\"âœ… Model appears to be saved correctly!\")\n    print(f\"   Use this path: {final_path}\")\nelif model_path.exists() and (model_path / \"config.json\").exists():\n    print(\"âœ… Model is saved in main directory!\")\n    print(f\"   Use this path: {model_path}\")\nelse:\n    print(\"âŒ Model was NOT saved properly!\")\n    print(\"   You need to run the training script again.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T18:43:00.194934Z","iopub.execute_input":"2026-01-29T18:43:00.195237Z","iopub.status.idle":"2026-01-29T18:43:00.210498Z","shell.execute_reply.started":"2026-01-29T18:43:00.195214Z","shell.execute_reply":"2026-01-29T18:43:00.209817Z"}},"outputs":[{"name":"stdout","text":"Checking model directory...\n============================================================\nâœ… Directory exists: /kaggle/working/emotion-recognition-model\n\nContents of /kaggle/working/emotion-recognition-model:\n------------------------------------------------------------\n  ğŸ“ checkpoint-2847/\n     - config.json\n     - model.safetensors\n     - optimizer.pt\n     - rng_state.pth\n     - scaler.pt\n     ... and 3 more files\n  ğŸ“ checkpoint-3285/\n     - config.json\n     - model.safetensors\n     - optimizer.pt\n     - rng_state.pth\n     - scaler.pt\n     ... and 3 more files\n  ğŸ“ final/\n     - config.json\n     - model.safetensors\n     - preprocessor_config.json\n     - training_args.bin\n  ğŸ“ runs/\n     - Jan29_17-16-50_d701060de0c2\n\n============================================================\nChecking for required model files:\n------------------------------------------------------------\n  âŒ config.json (MISSING!)\n  âŒ preprocessor_config.json (MISSING!)\n  âŒ model.safetensors (MISSING!)\n\nâœ… Found 'final' subdirectory!\n\nContents of /kaggle/working/emotion-recognition-model/final:\n------------------------------------------------------------\n  ğŸ“„ config.json (0.00 MB)\n  ğŸ“„ model.safetensors (1204.36 MB)\n  ğŸ“„ preprocessor_config.json (0.00 MB)\n  ğŸ“„ training_args.bin (0.01 MB)\n\n============================================================\nCONCLUSION:\n============================================================\nâœ… Model appears to be saved correctly!\n   Use this path: /kaggle/working/emotion-recognition-model/final\n","output_type":"stream"}],"execution_count":3}]}