# VCAI - Virtual Customer AI Training System

<div align="center">

![VCAI](https://img.shields.io/badge/VCAI-Virtual%20Customer%20AI-0066CC?style=for-the-badge&labelColor=000000)
![Version](https://img.shields.io/badge/version-1.0.0-green?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3.11-3776AB?style=for-the-badge&logo=python&logoColor=white)
![React](https://img.shields.io/badge/React-19-61DAFB?style=for-the-badge&logo=react&logoColor=black)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-009688?style=for-the-badge&logo=fastapi&logoColor=white)

*AI-powered sales training platform with real-time voice conversations in Egyptian Arabic*

[Overview](#-overview) Â· [Features](#-features) Â· [Demo](#-demo) Â· [Installation](#-installation) Â· [Architecture](#-architecture) Â· [API](#-api-documentation)

---

</div>

## ğŸ¯ Overview

VCAI (Virtual Customer AI) is an intelligent training platform designed for real estate sales professionals. It simulates realistic customer interactions in *Egyptian Arabic*, providing a safe environment to practice handling various customer personalities and scenarios.

### The Problem

Traditional sales training relies on role-playing with colleagues or managers, which is:
- *Inconsistent* - Different trainers provide different experiences
- *Limited* - Can't practice 24/7
- *Biased* - Colleagues may not act like real difficult customers
- *Expensive* - Requires dedicated training time from senior staff

### The Solution

VCAI provides an AI-powered virtual customer that:
- *Responds naturally* in Egyptian Arabic dialect
- *Adapts emotionally* based on the conversation flow
- *Simulates different personalities* from friendly to difficult customers
- *Provides instant feedback* on sales techniques
- *Available 24/7* for unlimited practice sessions

---

## âœ¨ Features

### Core Capabilities

| Feature | Technology | Status |
|---------|------------|--------|
| ğŸ¤ *Real-time Speech Recognition* | Faster-Whisper large-v3-turbo (GPU) | âœ… Working |
| ğŸ—£ï¸ *Egyptian Arabic TTS* | Chatterbox Multilingual, fine-tuned on Egyptian data | âœ… Working |
| ğŸ˜¤ *Emotion Detection* | Custom-trained emotion2vec + AraBERT fusion (96.8% accuracy) | âœ… Working |
| ğŸ¤– *Intelligent Responses* | Qwen 2.5-7B-Instruct, 4-bit quantized (BitsAndBytes NF4) | âœ… Working |
| ğŸ“š *Knowledge Retrieval* | ChromaDB + Sentence-Transformers RAG | ğŸŸ¡ In Progress |
| ğŸ§  *Conversation Memory* | PostgreSQL with automatic checkpointing every 5 turns | âœ… Working |
| ğŸ”Š *Streaming Audio* | LLMâ†’TTS sentence-level streaming for low perceived latency | âœ… Working |
| ğŸ‘¥ *Multiple Personas* | 5 distinct customer personalities | âœ… Working |

### Customer Personas

| Persona | Personality | Challenge Level |
|---------|-------------|-----------------|
| ğŸ§ *Price-Focused Customer* | Primarily concerned with getting the best deal | Medium |
| ğŸ˜¤ *Difficult Customer* | Skeptical, hard to please, raises objections | Hard |
| ğŸ˜Š *Friendly Customer* | Open and cooperative, easy to work with | Easy |
| â° *Rushed Customer* | Limited time, wants quick answers | Medium |
| ğŸ”¬ *Detail-Oriented Customer* | Asks many technical questions | Hard |

---

## ğŸ¬ Demo

### Training Session Flow


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Training Session                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Salesperson: "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…ØŒ Ù…Ø¹Ø§Ùƒ Ø£Ø­Ù…Ø¯ Ù…Ù† Ø´Ø±ÙƒØ© Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª"      â”‚
â”‚               (Hello, this is Ahmed from the real estate     â”‚
â”‚                company)                                      â”‚
â”‚                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                              â”‚
â”‚  ğŸ­ Customer (Price-Focused):                                â”‚
â”‚     "ÙˆØ¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù…ØŒ Ø£Ù†Ø§ Ø¹Ø§ÙŠØ² Ø£Ø¹Ø±Ù Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø´Ù‚Ù‚ Ø¹Ù†Ø¯ÙƒÙ…ØŒ         â”‚
â”‚      Ø¨Ø³ Ù…Ø´ Ø¹Ø§ÙŠØ² Ø­Ø§Ø¬Ø© ØºØ§Ù„ÙŠØ©"                                  â”‚
â”‚     (Hello, I want to know your apartment prices,           â”‚
â”‚      but I don't want anything expensive)                   â”‚
â”‚                                                              â”‚
â”‚  ğŸ“Š Emotion: Interested â”‚ Mood: 65% â”‚ Risk: Low             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


---

## ğŸ’» System Requirements

### Minimum Specifications

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| *OS* | Windows 10/11, Ubuntu 20.04+ | Windows 11, Ubuntu 22.04 |
| *CPU* | Intel i5 / AMD Ryzen 5 | Intel i7 / AMD Ryzen 7 |
| *RAM* | 16 GB | 32 GB |
| *GPU* | NVIDIA GTX 1060 (6GB VRAM) | NVIDIA RTX 3060+ (12GB VRAM) |
| *Storage* | 20 GB SSD | 40 GB SSD |
| *CUDA* | 12.1 | 12.1+ |
| *Python* | 3.11.x | 3.11.x |
| *Node.js* | 18.x | 20.x+ |

> âš ï¸ *GPU is required.* VCAI loads multiple models simultaneously (STT, LLM, TTS, Emotion) which require ~10GB VRAM total.

---

## ğŸš€ Installation

### Prerequisites

Ensure you have installed:
- [Anaconda](https://www.anaconda.com/download) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html)
- [PostgreSQL](https://www.postgresql.org/download/) (create a database named vcai)
- [Node.js 20+](https://nodejs.org/)
- [FFmpeg](https://ffmpeg.org/download.html)
- [CUDA Toolkit 12.1+](https://developer.nvidia.com/cuda-downloads)
- NVIDIA GPU drivers (latest)

### Step-by-Step Setup

bash
# 1. Clone the repository
git clone https://github.com/your-org/VCAI.git
cd VCAI

# 2. Create conda environment (Python 3.11 required for Chatterbox)
conda create -n vcai python=3.11 -y
conda activate vcai

# 3. Install Chatterbox TTS first (has specific dependency requirements)
pip install chatterbox-tts

# 4. Reinstall PyTorch with CUDA support (chatterbox may install CPU-only)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# 5. Install remaining project dependencies
pip install -r requirements.txt

# 6. Install frontend dependencies
cd frontend && npm install && cd ..

# 7. Start the application
# Terminal 1 â€” Backend
python -m backend.main

# Terminal 2 â€” Frontend
cd frontend && npm run dev


### Verify Installation

bash
# Check CUDA is available
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0)}')"

# Check bitsandbytes
python -c "import bitsandbytes; print('BitsAndBytes OK')"

# Check chatterbox
python -c "from chatterbox.mtl_tts import ChatterboxMultilingualTTS; print('Chatterbox OK')"


### Access the Application

Open your browser and navigate to: *http://localhost:5173*

---

## ğŸ—ï¸ Architecture

### System Overview


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Client Layer                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    React Frontend (Vite)                       â”‚ â”‚
â”‚  â”‚   Dashboard â”‚ Training Session â”‚ Session Setup â”‚ Login        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â”‚ WebSocket (streaming audio chunks)
                                   â”‚ REST API (auth, sessions, personas)
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         API Layer                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                   FastAPI Backend                              â”‚ â”‚
â”‚  â”‚   Authentication â”‚ Sessions â”‚ Personas â”‚ WebSocket Handler     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Orchestration Layer                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              LangGraph Pipeline (Streaming)                    â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚   â”‚ Memory  â”‚â”€â”€â”€â–¶â”‚   STT   â”‚â”€â”€â”€â–¶â”‚ Emotion â”‚â”€â”€â”€â–¶â”‚   RAG   â”‚   â”‚ â”‚
â”‚  â”‚   â”‚  Load   â”‚    â”‚ Whisper â”‚    â”‚ Fusion  â”‚    â”‚ ChromaDBâ”‚   â”‚ â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â”‚                                                      â”‚         â”‚ â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚         â”‚ â”‚
â”‚  â”‚   â”‚ Memory  â”‚â—€â”€â”€â”€â”‚   TTS   â”‚â—€â”€â”€â–¶â”‚   LLM   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”‚
â”‚  â”‚   â”‚  Save   â”‚    â”‚Chatter- â”‚    â”‚  Qwen   â”‚  (streaming)     â”‚ â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  box    â”‚    â”‚ 2.5-7B  â”‚                   â”‚ â”‚
â”‚  â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


### Streaming Pipeline

VCAI uses a sentence-level streaming architecture for low perceived latency:


User speaks â†’ STT (0.3s) â†’ Emotion (0.06s) â†’ RAG â†’ LLM starts generating
  â†’ Sentence 1 complete â†’ TTS chunk 1 â†’ Send to browser â†’ PLAY immediately
  â†’ Sentence 2 complete â†’ TTS chunk 2 â†’ Send to browser â†’ PLAY next
  â†’ ...
User hears first audio at ~2.5s instead of ~5.5s (55% faster perceived latency)


### Conversation Turn Pipeline

| Step | Component | Technology | Latency |
|------|-----------|------------|---------|
| 1 | *Memory Load* | PostgreSQL + checkpoints | ~5ms |
| 2 | *STT* | Faster-Whisper large-v3-turbo | ~300-450ms |
| 3 | *Emotion* | emotion2vec + AraBERT fusion | ~55-60ms |
| 4 | *RAG* | ChromaDB + sentence-transformers | ~100ms |
| 5 | *LLM* | Qwen 2.5-7B (4-bit NF4) | ~1-3s (streamed) |
| 6 | *TTS* | Chatterbox Multilingual (Egyptian fine-tuned) | ~1.5-3s per chunk |
| 7 | *Memory Save* | PostgreSQL + LLM summarization | ~5ms (8s on checkpoint) |

---

## ğŸ“ Project Structure


VCAI/
â”œâ”€â”€ backend/                    # FastAPI Backend
â”‚   â”œâ”€â”€ main.py                # Entry point + ML model preloading
â”‚   â”œâ”€â”€ config.py              # Configuration management
â”‚   â”œâ”€â”€ database.py            # Database connection
â”‚   â”œâ”€â”€ models/                # SQLAlchemy ORM models
â”‚   â”œâ”€â”€ routers/               # API routes + WebSocket handler
â”‚   â”œâ”€â”€ schemas/               # Pydantic validation schemas
â”‚   â””â”€â”€ services/              # Business logic services
â”‚
â”œâ”€â”€ frontend/                   # React Frontend (Vite)
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ components/        # Reusable UI components
â”‚       â”œâ”€â”€ pages/             # Page components
â”‚       â”‚   â”œâ”€â”€ TrainingSession.jsx  # Main training UI + audio streaming
â”‚       â”‚   â”œâ”€â”€ SessionSetup.jsx     # Persona selection
â”‚       â”‚   â”œâ”€â”€ Dashboard.jsx        # Session history
â”‚       â”‚   â””â”€â”€ Login.jsx / Register.jsx
â”‚       â”œâ”€â”€ context/           # Auth context provider
â”‚       â””â”€â”€ services/          # API + WebSocket client
â”‚
â”œâ”€â”€ orchestration/              # LangGraph Orchestration
â”‚   â”œâ”€â”€ agent.py               # Main orchestration agent
â”‚   â”œâ”€â”€ state.py               # ConversationState (TypedDict)
â”‚   â”œâ”€â”€ config.py              # Pipeline configuration
â”‚   â”œâ”€â”€ graphs/                # LangGraph workflow definitions
â”‚   â”‚   â””â”€â”€ conversation_graph.py  # Main pipeline graph
â”‚   â””â”€â”€ nodes/                 # Individual pipeline nodes
â”‚       â”œâ”€â”€ stt_node.py
â”‚       â”œâ”€â”€ emotion_node.py
â”‚       â”œâ”€â”€ rag_node.py
â”‚       â”œâ”€â”€ llm_node.py        # + llm_node_streaming() generator
â”‚       â”œâ”€â”€ tts_node.py        # + tts_chunk() for streaming
â”‚       â””â”€â”€ memory_node.py     # load + save
â”‚
â”œâ”€â”€ stt/                        # Speech-to-Text
â”‚   â””â”€â”€ realtime_stt.py        # Faster-Whisper implementation
â”‚
â”œâ”€â”€ tts/                        # Text-to-Speech
â”‚   â”œâ”€â”€ agent.py               # TTS interface + Egyptian checkpoint loading
â”‚   â””â”€â”€ chatterbox_model.py    # Chatterbox wrapper class
â”‚
â”œâ”€â”€ emotion/                    # Emotion Detection
â”‚   â”œâ”€â”€ agent.py               # Emotion analysis orchestrator
â”‚   â”œâ”€â”€ voice_emotion.py       # emotion2vec voice classifier
â”‚   â”œâ”€â”€ text_emotion.py        # AraBERT text sentiment
â”‚   â””â”€â”€ fusion.py              # Voice + text emotion fusion
â”‚
â”œâ”€â”€ llm/                        # Language Model
â”‚   â”œâ”€â”€ agent.py               # Qwen 2.5-7B with streaming support
â”‚   â””â”€â”€ prompts.py             # System prompt templates
â”‚
â”œâ”€â”€ rag/                        # Retrieval-Augmented Generation
â”‚   â”œâ”€â”€ agent.py               # RAG interface
â”‚   â”œâ”€â”€ embeddings.py          # Embedding model
â”‚   â”œâ”€â”€ vector_store.py        # ChromaDB operations
â”‚   â””â”€â”€ document_loader.py     # Document ingestion
â”‚
â”œâ”€â”€ memory/                     # Conversation Memory
â”‚   â”œâ”€â”€ agent.py               # Memory interface
â”‚   â””â”€â”€ store.py               # PostgreSQL CRUD operations
â”‚
â”œâ”€â”€ persona/                    # Customer Personas
â”‚   â””â”€â”€ agent.py               # Persona management + prompts
â”‚
â”œâ”€â”€ shared/                     # Shared Utilities
â”‚   â”œâ”€â”€ types.py               # TypedDict definitions
â”‚   â”œâ”€â”€ constants.py           # Application constants
â”‚   â””â”€â”€ interfaces.py          # Function signatures
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md


---

## ğŸ“š API Documentation

### Interactive Documentation

When the backend is running, access the interactive API docs at:
- *Swagger UI:* http://localhost:8000/docs
- *ReDoc:* http://localhost:8000/redoc

### REST Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | /api/auth/register | Register new user |
| POST | /api/auth/login | Authenticate user |
| GET | /api/auth/me | Get current user profile |
| GET | /api/personas | List available personas |
| GET | /api/personas/{id} | Get persona details |
| POST | /api/sessions | Create training session |
| GET | /api/sessions | List user sessions |
| GET | /api/sessions/{id} | Get session details |

### WebSocket Protocol

*Endpoint:* ws://localhost:8000/ws/{session_id}?token={jwt_token}

#### Client â†’ Server

json
{ "type": "audio_complete", "data": { "audio_base64": "...", "format": "webm" } }

json
{ "type": "end_session" }


#### Server â†’ Client

json
{ "type": "transcription", "data": { "text": "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…" } }

json
{ "type": "audio_chunk", "data": { "audio_base64": "...", "sample_rate": 24000, "chunk_index": 1, "text": "ÙˆØ¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù…", "is_final": false } }

json
{ "type": "audio_chunk", "data": { "is_final": true, "total_chunks": 2 } }

json
{ "type": "response", "data": { "text": "Full response text" } }

json
{ "type": "emotion", "data": { "emotion": "interested", "mood_score": 65, "risk_level": "low", "tip": "..." } }


---

## âš™ï¸ Configuration

### Environment Variables

Create a .env file in the project root:

env
# Database (PostgreSQL required)
DATABASE_URL=postgresql://postgres:yourpassword@localhost:5432/vcai

# Security
SECRET_KEY=your-secure-secret-key-here
JWT_ALGORITHM=HS256
JWT_EXPIRATION_HOURS=24

# Feature Flags
USE_MOCKS=false
DEBUG=false


> âš ï¸ *PostgreSQL is required.* Install [PostgreSQL](https://www.postgresql.org/download/) and create a database named vcai before running the app. The SQL schema is in scripts/setup_db.sql.

### TTS Fine-tuned Checkpoint

The TTS uses an Egyptian Arabic fine-tuned checkpoint. Configure the path in tts/agent.py:

python
EGYPTIAN_CHECKPOINT = r"C:\path\to\checkpoint-2000\model.safetensors"
# Set to None to use base Chatterbox model


---

## ğŸ”§ Troubleshooting

<details>
<summary><b>ğŸ”´ Chatterbox install fails (pkuseg error)</b></summary>

This is a known issue with chatterbox-tts >= 0.1.3. Fix:
bash
pip install --upgrade pip setuptools wheel cython
pip install numpy
pip install --no-build-isolation pkuseg
pip install chatterbox-tts

</details>

<details>
<summary><b>ğŸ”´ CUDA not detected after install</b></summary>

Chatterbox may install CPU-only PyTorch. Reinstall:
bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

</details>

<details>
<summary><b>ğŸ”´ bcrypt / chromadb dependency conflict</b></summary>

Install bcrypt before chromadb:
bash
pip install bcrypt==4.0.1
pip install chromadb

</details>

<details>
<summary><b>ğŸ”´ bitsandbytes CUDA errors on Windows</b></summary>

Use the latest version which has native Windows support:
bash
pip install bitsandbytes>=0.45.0

</details>

<details>
<summary><b>ğŸ”´ Microphone not transcribing accurately</b></summary>

Increase microphone volume and enable boost in Windows sound settings. The system normalizes quiet audio automatically, but very low input levels may still cause issues.
</details>

---

## ğŸ“ˆ Performance Metrics

### Benchmarks (NVIDIA RTX â€” tested)

| Metric | First Turn | Subsequent Turns |
|--------|-----------|-----------------|
| STT Latency | ~1.1s (cold start) | 0.25-0.45s |
| Emotion Analysis | ~0.7s (model load) | 0.05-0.06s |
| LLM Response | 2-3s | 1-3s |
| TTS per Chunk | 1.5-3s | 1.2-2.5s |
| Memory Load/Save | 50ms / 5ms | 3-5ms / 5ms |
| *First Audio Heard* | *~4s* | *~2.5s* |
| *Total Turn Time* | ~6.5s | 2.5-5s |

---

## ğŸ›£ï¸ Roadmap

- [x] Core conversation pipeline (LangGraph orchestration)
- [x] Real-time speech recognition (Faster-Whisper GPU)
- [x] Emotion detection (custom-trained emotion2vec + AraBERT fusion)
- [x] LLM integration (Qwen 2.5-7B, 4-bit quantized)
- [x] Egyptian Arabic TTS (Chatterbox fine-tuned)
- [x] Streaming audio pipeline (sentence-level LLMâ†’TTS)
- [x] Conversation memory with checkpoints
- [x] WebSocket real-time communication
- [ ] RAG with property database (ChromaDB)
- [ ] Performance analytics dashboard
- [ ] Post-session evaluation and scoring
- [ ] Mobile application

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [Faster-Whisper](https://github.com/SYSTRAN/faster-whisper) â€” Speech recognition
- [Chatterbox TTS](https://github.com/resemble-ai/chatterbox) â€” Text-to-speech
- [Qwen 2.5](https://github.com/QwenLM/Qwen2.5) â€” Language model
- [LangGraph](https://github.com/langchain-ai/langgraph) â€” Pipeline orchestration
- [FastAPI](https://fastapi.tiangolo.com/) â€” Backend framework
- [React](https://react.dev/) â€” Frontend framework

---

<div align="center">

*Built with â¤ï¸ for sales excellence*

</div>